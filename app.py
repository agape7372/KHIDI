# -*- coding: utf-8 -*-
"""
KHIDI AI ì±„ìš© ë¹„ì„œ ëŒ€ì‹œë³´ë“œ
í•œêµ­ë³´ê±´ì‚°ì—…ì§„í¥ì› ì·¨ì—… ì¤€ë¹„ìƒì„ ìœ„í•œ ì¸ë°”ìŠ¤ì¼“ ë¶„ì„ ë„êµ¬
"""

import streamlit as st
import sqlite3
import requests
from bs4 import BeautifulSoup
import pdfplumber
import google.generativeai as genai
from datetime import datetime, timedelta
import os
import tempfile
import hashlib
import json
import re
from typing import Optional, List, Dict, Tuple

# ============================================================
# ì„¤ì • ìƒìˆ˜
# ============================================================
DB_PATH = "khidi_data.db"
PDF_CACHE_DIR = "pdf_cache"

KHIDI_URLS = {
    "ë³´ê±´ì‚°ì—…ë¸Œë¦¬í”„": "https://www.khidi.or.kr/board?menuId=MENU00085",
    "ê¸€ë¡œë²Œë³´ê±´ì‚°ì—…ë™í–¥": "https://www.khidi.or.kr/board?menuId=MENU00949",
    "ë‰´ìŠ¤ë ˆí„°": "https://www.khidi.or.kr/board?menuId=MENU00094",
}

CATEGORIES = ["ì „ì²´", "R&D ì •ì±…", "ê¸€ë¡œë²Œ ì§„ì¶œ", "ê·œì œ/ë²•ë ¹", "ì±„ìš© ë¶„ì„"]

# ============================================================
# ë°ì´í„°ë² ì´ìŠ¤ í•¨ìˆ˜
# ============================================================
def init_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # ë¸Œë¦¬í•‘ ê²Œì‹œê¸€ í…Œì´ë¸”
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS briefings (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT NOT NULL,
            source TEXT,
            category TEXT,
            url TEXT UNIQUE,
            pdf_url TEXT,
            content TEXT,
            ai_analysis TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    # ì±„ìš© ê³µê³  í…Œì´ë¸”
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS recruitments (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            year INTEGER,
            position TEXT,
            department TEXT,
            requirements TEXT,
            skills TEXT,
            hired_count INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    conn.commit()
    conn.close()

    # ë”ë¯¸ ì±„ìš© ë°ì´í„° ì‚½ì…
    insert_dummy_recruitment_data()

def insert_dummy_recruitment_data():
    """2021~2025ë…„ ëª¨ì˜ ì±„ìš© ë°ì´í„° ì‚½ì…"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # ì´ë¯¸ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
    cursor.execute("SELECT COUNT(*) FROM recruitments")
    if cursor.fetchone()[0] > 0:
        conn.close()
        return

    dummy_data = [
        # 2021ë…„
        (2021, "ë³´ê±´ì‚°ì—… ì •ì±…ì—°êµ¬ì›", "ì •ì±…ì—°êµ¬ë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, ë³´ê±´ì •ì±… ì „ê³µ", "ì •ì±…ë¶„ì„, í†µê³„ë¶„ì„, ë³´ê³ ì„œ ì‘ì„±", 3),
        (2021, "R&D ì‚¬ì—…ê´€ë¦¬", "R&Dì‚¬ì—…ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, ì´ê³µê³„ì—´", "ì‚¬ì—…ê´€ë¦¬, ì˜ˆì‚°í¸ì„±, ì„±ê³¼í‰ê°€", 5),
        (2021, "í–‰ì •ì§€ì›", "ê²½ì˜ì§€ì›ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ", "ë¬¸ì„œê´€ë¦¬, íšŒê³„, ì¸ì‚¬", 2),

        # 2022ë…„
        (2022, "ë°”ì´ì˜¤í—¬ìŠ¤ ì‚¬ì—…ê´€ë¦¬", "ë°”ì´ì˜¤í—¬ìŠ¤ì‚°ì—…ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, ìƒëª…ê³¼í•™/ì˜ê³µí•™", "ì„ìƒì‹œí—˜ ê´€ë¦¬, ì¸í—ˆê°€ ì§€ì›", 4),
        (2022, "ê¸€ë¡œë²Œ ì§„ì¶œ ì§€ì›", "í•´ì™¸ì‚¬ì—…ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, ì˜ì–´ ëŠ¥í†µ", "í•´ì™¸ì‹œì¥ ì¡°ì‚¬, ìˆ˜ì¶œ ì§€ì›", 3),
        (2022, "ë°ì´í„° ë¶„ì„ê°€", "ì •ì±…ì—°êµ¬ë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, í†µê³„/ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "ë¹…ë°ì´í„° ë¶„ì„, AI ëª¨ë¸ë§", 2),

        # 2023ë…„
        (2023, "ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´ PM", "ë””ì§€í„¸í—¬ìŠ¤ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, IT/ì˜ë£Œ ìœµí•©", "ë””ì§€í„¸ì¹˜ë£Œì œ, AIì˜ë£Œê¸°ê¸° ê´€ë¦¬", 6),
        (2023, "ê·œì œí˜ì‹  ì „ë¬¸ê°€", "ê·œì œí˜ì‹ íŒ€", "í•™ì‚¬ ì´ìƒ, ë²•í•™/ë³´ê±´í•™", "ê·œì œìƒŒë“œë°•ìŠ¤, ì¸í—ˆê°€ ì»¨ì„¤íŒ…", 2),
        (2023, "ì˜ë£Œê¸°ê¸° ì‚¬ì—…ê´€ë¦¬", "ì˜ë£Œê¸°ê¸°ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, ì˜ê³µí•™/ê¸°ê³„ê³µí•™", "ì˜ë£Œê¸°ê¸° ì¸ì¦, í’ˆì§ˆê´€ë¦¬", 4),

        # 2024ë…„
        (2024, "ë°”ì´ì˜¤ì˜ì•½í’ˆ PM", "ë°”ì´ì˜¤ì˜ì•½í’ˆë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, ì•½í•™/ìƒëª…ê³¼í•™", "ë°”ì´ì˜¤ì‹œë°€ëŸ¬, ì„¸í¬ì¹˜ë£Œì œ ê´€ë¦¬", 5),
        (2024, "AI í—¬ìŠ¤ì¼€ì–´ ì „ë¬¸ê°€", "ë””ì§€í„¸í—¬ìŠ¤ë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, AI/ML ì „ê³µ", "AI ì§„ë‹¨, ë””ì§€í„¸ë°”ì´ì˜¤ë§ˆì»¤", 3),
        (2024, "ê¸€ë¡œë²Œ ì„ìƒ ì§€ì›", "í•´ì™¸ì‚¬ì—…ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, ì„ìƒ ê²½í—˜ì", "ê¸€ë¡œë²Œ ì„ìƒì‹œí—˜, FDA/EMA ëŒ€ì‘", 2),
        (2024, "ESG ê²½ì˜ ë‹´ë‹¹", "ê²½ì˜ì§€ì›ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ", "ESG ì „ëµ, ì§€ì†ê°€ëŠ¥ê²½ì˜ ë³´ê³ ì„œ", 1),

        # 2025ë…„
        (2025, "ì²¨ë‹¨ë°”ì´ì˜¤ ì‚¬ì—…ê´€ë¦¬", "ì²¨ë‹¨ë°”ì´ì˜¤ë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, ìœ ì „ì²´í•™/í•©ì„±ìƒë¬¼í•™", "ìœ ì „ìì¹˜ë£Œ, mRNA í”Œë«í¼", 4),
        (2025, "ë””ì§€í„¸ì¹˜ë£Œì œ PM", "ë””ì§€í„¸í—¬ìŠ¤ë³¸ë¶€", "í•™ì‚¬ ì´ìƒ, SW/ì˜ë£Œ ìœµí•©", "DTx ì¸í—ˆê°€, ì„ìƒ ì„¤ê³„", 3),
        (2025, "ë³´ê±´ì•ˆë³´ ì „ë¬¸ê°€", "ë³´ê±´ì•ˆë³´íŒ€", "ì„ì‚¬ ì´ìƒ, ê³µì¤‘ë³´ê±´/ê°ì—¼ë³‘", "íŒ¬ë°ë¯¹ ëŒ€ì‘, ë°±ì‹  ìˆ˜ê¸‰", 2),
        (2025, "ë©”ë””ì»¬ ë¼ì´í„°", "ì •ì±…ì—°êµ¬ë³¸ë¶€", "ì„ì‚¬ ì´ìƒ, ì˜í•™/ì•½í•™", "ë³´ê±´ì‚°ì—… ë°±ì„œ, ì •ì±…ë³´ê³ ì„œ", 2),
    ]

    cursor.executemany("""
        INSERT INTO recruitments (year, position, department, requirements, skills, hired_count)
        VALUES (?, ?, ?, ?, ?, ?)
    """, dummy_data)

    conn.commit()
    conn.close()

def save_briefing(title: str, source: str, category: str, url: str,
                  pdf_url: str = None, content: str = None, ai_analysis: str = None):
    """ë¸Œë¦¬í•‘ ë°ì´í„° ì €ì¥"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    try:
        cursor.execute("""
            INSERT OR REPLACE INTO briefings
            (title, source, category, url, pdf_url, content, ai_analysis, crawled_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (title, source, category, url, pdf_url, content, ai_analysis, datetime.now()))
        conn.commit()
    except Exception as e:
        st.error(f"DB ì €ì¥ ì˜¤ë¥˜: {e}")
    finally:
        conn.close()

def get_briefings(category: str = "ì „ì²´", limit: int = 20) -> List[Dict]:
    """ë¸Œë¦¬í•‘ ë°ì´í„° ì¡°íšŒ"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    if category == "ì „ì²´":
        cursor.execute("""
            SELECT * FROM briefings
            ORDER BY crawled_at DESC LIMIT ?
        """, (limit,))
    else:
        cursor.execute("""
            SELECT * FROM briefings
            WHERE category = ?
            ORDER BY crawled_at DESC LIMIT ?
        """, (category, limit))

    columns = [desc[0] for desc in cursor.description]
    results = [dict(zip(columns, row)) for row in cursor.fetchall()]
    conn.close()
    return results

def get_recruitment_data() -> List[Dict]:
    """ì±„ìš© ë°ì´í„° ì¡°íšŒ"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM recruitments ORDER BY year DESC, position")
    columns = [desc[0] for desc in cursor.description]
    results = [dict(zip(columns, row)) for row in cursor.fetchall()]
    conn.close()
    return results

# ============================================================
# í¬ë¡¤ëŸ¬ í•¨ìˆ˜
# ============================================================
def crawl_khidi_board(board_name: str, board_url: str, max_items: int = 5) -> List[Dict]:
    """KHIDI ê²Œì‹œíŒ í¬ë¡¤ë§"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    try:
        response = requests.get(board_url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        articles = []

        # KHIDI ê²Œì‹œíŒ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹± (ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡°)
        rows = soup.select('table tbody tr, .board-list li, .list-item')

        for row in rows[:max_items]:
            try:
                # ì œëª© ì¶”ì¶œ
                title_elem = row.select_one('a, .title, .subject')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                link = title_elem.get('href', '')

                if link and not link.startswith('http'):
                    link = f"https://www.khidi.or.kr{link}"

                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = row.select_one('.date, .regdate, td:last-child')
                date_str = date_elem.get_text(strip=True) if date_elem else ""

                articles.append({
                    "title": title,
                    "url": link,
                    "date": date_str,
                    "source": board_name
                })
            except Exception as e:
                continue

        return articles

    except Exception as e:
        st.warning(f"{board_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

def get_article_detail(url: str) -> Tuple[str, Optional[str]]:
    """ê²Œì‹œê¸€ ìƒì„¸ ë‚´ìš© ë° PDF URL ì¶”ì¶œ"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        content_elem = soup.select_one('.board-view-content, .content, .view-content, article')
        content = content_elem.get_text(strip=True) if content_elem else ""

        # PDF ë§í¬ ì¶”ì¶œ
        pdf_url = None
        for link in soup.select('a[href*=".pdf"], a[href*="download"]'):
            href = link.get('href', '')
            if '.pdf' in href.lower() or 'download' in href.lower():
                if not href.startswith('http'):
                    href = f"https://www.khidi.or.kr{href}"
                pdf_url = href
                break

        return content, pdf_url

    except Exception as e:
        return "", None

def download_and_extract_pdf(pdf_url: str) -> str:
    """PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not pdf_url:
        return ""

    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(PDF_CACHE_DIR, exist_ok=True)

    # URL í•´ì‹œë¡œ ìºì‹œ íŒŒì¼ëª… ìƒì„±
    url_hash = hashlib.md5(pdf_url.encode()).hexdigest()
    cache_path = os.path.join(PDF_CACHE_DIR, f"{url_hash}.txt")

    # ìºì‹œ í™•ì¸
    if os.path.exists(cache_path):
        with open(cache_path, 'r', encoding='utf-8') as f:
            return f.read()

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        response = requests.get(pdf_url, headers=headers, timeout=30)

        if response.status_code != 200:
            return ""

        # ì„ì‹œ íŒŒì¼ì— PDF ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(response.content)
            tmp_path = tmp_file.name

        # pdfplumberë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_content = []
        try:
            with pdfplumber.open(tmp_path) as pdf:
                for page in pdf.pages[:20]:  # ìµœëŒ€ 20í˜ì´ì§€ë§Œ ì²˜ë¦¬
                    page_text = page.extract_text()
                    if page_text:
                        text_content.append(page_text)
        finally:
            os.unlink(tmp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

        full_text = "\n".join(text_content)

        # ìºì‹œì— ì €ì¥
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(full_text)

        return full_text

    except Exception as e:
        st.warning(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return ""

# ============================================================
# AI ë¶„ì„ í•¨ìˆ˜ (Gemini API)
# ============================================================
def configure_gemini(api_key: str):
    """Gemini API ì„¤ì •"""
    genai.configure(api_key=api_key)

def generate_inbasket_analysis(content: str, title: str, api_key: str) -> str:
    """ì¸ë°”ìŠ¤ì¼“ í˜•ì‹ì˜ AI ë¶„ì„ ìƒì„±"""
    if not api_key:
        return "âš ï¸ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    if not content or len(content) < 100:
        return "âš ï¸ ë¶„ì„í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    try:
        configure_gemini(api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')

        # ì½˜í…ì¸ ê°€ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        if len(content) > 15000:
            content = content[:15000] + "\n...(ì´í•˜ ìƒëµ)"

        prompt = f"""
ë‹¹ì‹ ì€ í•œêµ­ë³´ê±´ì‚°ì—…ì§„í¥ì›(KHIDI) R&D ì‚¬ì—…ì§€ì›ë¶€ë¬¸ 3ë…„ ì°¨ ì£¼ì„ì…ë‹ˆë‹¤.
ì•„ë˜ ë³´ê±´ì‚°ì—… ê´€ë ¨ ìë£Œë¥¼ ì½ê³ , ì…ì‚¬ ì‹œí—˜ì¸ 'ì¸ë°”ìŠ¤ì¼“(In-Basket)' ë‹µì•ˆ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

[ìë£Œ ì œëª©]: {title}

[ìë£Œ ë‚´ìš©]:
{content}

---

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:

## ğŸ“‹ í˜„í™© ë° ë°°ê²½
(ì‚°ì—… ìˆ˜ì¹˜, ì •ì±… ê¸°ì¡°, ì‹œì¥ ë™í–¥ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½)

## âš ï¸ í•µì‹¬ ë¬¸ì œì 
(ê·œì œ ì¥ë²½, ì¸ë ¥ ë¶€ì¡±, ê¸°ìˆ  ê²©ì°¨ ë“± ì£¼ìš” ê°ˆë“± ìš”ì†Œë¥¼ ë¶ˆë¦¿ í¬ì¸íŠ¸ë¡œ 3ê°œ ë‚´ì™¸ ë„ì¶œ)

## ğŸ’¡ ëŒ€ì‘ ë°©ì•ˆ
### ë‹¨ê¸° (6ê°œì›” ì´ë‚´)
(KHIDI ì‹¤ë¬´ì ê´€ì ì—ì„œ ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©ì•ˆ 2ê°œ)

### ì¤‘ê¸° (1-2ë…„)
(ì •ì±… ì œì•ˆ ë˜ëŠ” ì‚¬ì—… ê¸°íš ê´€ì ì˜ ë°©ì•ˆ 2ê°œ)

## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼
### ì •ëŸ‰ì  ì„±ê³¼
(ìˆ˜ì¹˜ë¡œ í‘œí˜„ ê°€ëŠ¥í•œ ì˜ˆìƒ ì„±ê³¼)

### ì •ì„±ì  ì„±ê³¼
(ì§ˆì  ê°œì„  íš¨ê³¼)

---
ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‹¤ì œ KHIDI ì§ì›ì´ ì‘ì„±í•œ ê²ƒì²˜ëŸ¼ ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

        response = model.generate_content(prompt)
        return response.text

    except Exception as e:
        return f"âš ï¸ AI ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}"

def predict_future_jobs(api_key: str) -> str:
    """2026ë…„ ì±„ìš© ìœ ë§ ì§ë¬´ ì˜ˆì¸¡"""
    if not api_key:
        return "âš ï¸ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        configure_gemini(api_key)
        model = genai.GenerativeModel('gemini-1.5-flash')

        prompt = """
ë‹¹ì‹ ì€ í•œêµ­ë³´ê±´ì‚°ì—…ì§„í¥ì›(KHIDI) ì¸ì‚¬ë‹´ë‹¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
2025ë…„ ë³´ê±´ì‚°ì—… ë°±ì„œ, ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´ ì •ì±… ë™í–¥, ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ì „ëµì„ ê¸°ë°˜ìœ¼ë¡œ
2026ë…„ KHIDIì—ì„œ ì‹ ê·œ ì±„ìš©ì´ ì˜ˆìƒë˜ëŠ” ìœ ë§ ì§ë¬´ë¥¼ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:

## ğŸ”® 2026ë…„ KHIDI ìœ ë§ ì±„ìš© ì§ë¬´ ì˜ˆì¸¡

### 1ìˆœìœ„: [ì§ë¬´ëª…]
- **ì˜ˆìƒ ë¶€ì„œ**:
- **í•„ìš” ì—­ëŸ‰**:
- **ì±„ìš© ê·¼ê±°**: (ì–´ë–¤ ì •ì±…/ì‚°ì—… íŠ¸ë Œë“œ ë•Œë¬¸ì¸ì§€)

### 2ìˆœìœ„: [ì§ë¬´ëª…]
- **ì˜ˆìƒ ë¶€ì„œ**:
- **í•„ìš” ì—­ëŸ‰**:
- **ì±„ìš© ê·¼ê±°**:

### 3ìˆœìœ„: [ì§ë¬´ëª…]
- **ì˜ˆìƒ ë¶€ì„œ**:
- **í•„ìš” ì—­ëŸ‰**:
- **ì±„ìš© ê·¼ê±°**:

### ğŸ“š ì·¨ì—… ì¤€ë¹„ TIP
(í•´ë‹¹ ì§ë¬´ ì¤€ë¹„ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ì¡°ì–¸ 3ê°€ì§€)

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‹¤ì œ ë³´ê±´ì‚°ì—… íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•˜ì—¬ í˜„ì‹¤ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""

        response = model.generate_content(prompt)
        return response.text

    except Exception as e:
        return f"âš ï¸ ì˜ˆì¸¡ ìƒì„± ì‹¤íŒ¨: {e}"

def categorize_content(title: str, content: str) -> str:
    """ì½˜í…ì¸  ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
    text = (title + " " + content).lower()

    if any(kw in text for kw in ['r&d', 'ì—°êµ¬ê°œë°œ', 'ê¸°ìˆ ê°œë°œ', 'ì—°êµ¬ë¹„', 'ê³¼ì œ']):
        return "R&D ì •ì±…"
    elif any(kw in text for kw in ['ê¸€ë¡œë²Œ', 'í•´ì™¸', 'ìˆ˜ì¶œ', 'ì§„ì¶œ', 'fda', 'ema', 'êµ­ì œ']):
        return "ê¸€ë¡œë²Œ ì§„ì¶œ"
    elif any(kw in text for kw in ['ê·œì œ', 'ë²•ë ¹', 'ì¸í—ˆê°€', 'ìŠ¹ì¸', 'ì œë„', 'ë²•ë¥ ']):
        return "ê·œì œ/ë²•ë ¹"
    elif any(kw in text for kw in ['ì±„ìš©', 'ì¸ì¬', 'ì¼ìë¦¬', 'ì·¨ì—…', 'ê³ ìš©']):
        return "ì±„ìš© ë¶„ì„"
    else:
        return "R&D ì •ì±…"  # ê¸°ë³¸ê°’

# ============================================================
# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ìš©)
# ============================================================
def get_sample_briefings() -> List[Dict]:
    """ìƒ˜í”Œ ë¸Œë¦¬í•‘ ë°ì´í„°"""
    return [
        {
            "id": 1,
            "title": "2025ë…„ ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ê¸€ë¡œë²Œ ê²½ìŸë ¥ ê°•í™” ì „ëµ",
            "source": "ë³´ê±´ì‚°ì—…ë¸Œë¦¬í”„",
            "category": "R&D ì •ì±…",
            "url": "https://www.khidi.or.kr/sample1",
            "content": """
            2025ë…„ ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì€ ê¸€ë¡œë²Œ ì‹œì¥ ê·œëª¨ 3ì¡° ë‹¬ëŸ¬ë¥¼ ëŒíŒŒí•  ì „ë§ì´ë‹¤.
            í•œêµ­ì€ ë°”ì´ì˜¤ì‹œë°€ëŸ¬ ë¶„ì•¼ì—ì„œ ì„¸ê³„ 2ìœ„ì˜ ì‹œì¥ ì ìœ ìœ¨ì„ ê¸°ë¡í•˜ê³  ìˆìœ¼ë©°,
            ì„¸í¬Â·ìœ ì „ìì¹˜ë£Œì œ ë¶„ì•¼ì—ì„œë„ ê¸‰ì„±ì¥í•˜ê³  ìˆë‹¤.

            ì£¼ìš” ì •ì±… ë°©í–¥:
            1. ë°”ì´ì˜¤ì˜ì•½í’ˆ R&D íˆ¬ì í™•ëŒ€ (ì—°ê°„ 2ì¡°ì› ê·œëª¨)
            2. ê·œì œ ìƒŒë“œë°•ìŠ¤ë¥¼ í†µí•œ ì‹ ì† ì¸í—ˆê°€ ì§€ì›
            3. ê¸€ë¡œë²Œ ì„ìƒ ë„¤íŠ¸ì›Œí¬ êµ¬ì¶•
            4. ë°”ì´ì˜¤ ì¸ë ¥ ì–‘ì„± í”„ë¡œê·¸ë¨ í™•ëŒ€

            ì‚°ì—…ê³„ í˜„í™©:
            - êµ­ë‚´ ë°”ì´ì˜¤ê¸°ì—… ìˆ˜: 1,200ê°œ ì´ìƒ
            - ë°”ì´ì˜¤í—¬ìŠ¤ ìˆ˜ì¶œì•¡: 200ì–µ ë‹¬ëŸ¬ (ì „ë…„ ëŒ€ë¹„ 15% ì¦ê°€)
            - R&D íˆ¬ì ë¹„ì¤‘: ë§¤ì¶œ ëŒ€ë¹„ í‰ê·  12%
            """,
            "ai_analysis": None,
            "crawled_at": datetime.now().isoformat()
        },
        {
            "id": 2,
            "title": "ë””ì§€í„¸ì¹˜ë£Œì œ(DTx) ì‚°ì—… ë™í–¥ ë° ì •ì±… ê³¼ì œ",
            "source": "ê¸€ë¡œë²Œë³´ê±´ì‚°ì—…ë™í–¥",
            "category": "R&D ì •ì±…",
            "url": "https://www.khidi.or.kr/sample2",
            "content": """
            ë””ì§€í„¸ì¹˜ë£Œì œ(Digital Therapeutics)ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆë°©,
            ê´€ë¦¬, ì¹˜ë£Œí•˜ëŠ” ìƒˆë¡œìš´ ì˜ë£Œ íŒ¨ëŸ¬ë‹¤ì„ì´ë‹¤.

            ê¸€ë¡œë²Œ ì‹œì¥ í˜„í™©:
            - 2025ë…„ ì‹œì¥ ê·œëª¨: 89ì–µ ë‹¬ëŸ¬
            - ì—°í‰ê·  ì„±ì¥ë¥ : 25.4%
            - ì£¼ìš” ì ìš© ë¶„ì•¼: ì •ì‹ ê±´ê°•, ë‹¹ë‡¨ê´€ë¦¬, í˜¸í¡ê¸°ì§ˆí™˜

            êµ­ë‚´ í˜„í™© ë° ê³¼ì œ:
            1. êµ­ë‚´ DTx ê°œë°œ ê¸°ì—…: 50ê°œ ì´ìƒ
            2. ì„ìƒì‹œí—˜ ì§„í–‰ ì¤‘ì¸ ì œí’ˆ: 30ê°œ ì´ìƒ
            3. ê±´ê°•ë³´í—˜ ê¸‰ì—¬ ì ìš© ë…¼ì˜ ì§„í–‰ ì¤‘

            ì •ì±… ì œì–¸:
            - DTx ì „ìš© ì¸í—ˆê°€ íŠ¸ë™ ë§ˆë ¨
            - ì˜ë£Œë°ì´í„° í™œìš© ê·œì œ ì™„í™”
            - ìˆ˜ê°€ ì²´ê³„ ë° ê¸‰ì—¬ ê¸°ì¤€ ìˆ˜ë¦½
            """,
            "ai_analysis": None,
            "crawled_at": datetime.now().isoformat()
        },
        {
            "id": 3,
            "title": "ë¯¸êµ­ FDA ì˜ë£Œê¸°ê¸° ì¸í—ˆê°€ ë™í–¥ ë¶„ì„",
            "source": "ê¸€ë¡œë²Œë³´ê±´ì‚°ì—…ë™í–¥",
            "category": "ê¸€ë¡œë²Œ ì§„ì¶œ",
            "url": "https://www.khidi.or.kr/sample3",
            "content": """
            ë¯¸êµ­ FDAì˜ ì˜ë£Œê¸°ê¸° ì¸í—ˆê°€ ì •ì±… ë³€í™”ì™€ êµ­ë‚´ ê¸°ì—…ì˜ ëŒ€ì‘ ì „ëµì„ ë¶„ì„í•œë‹¤.

            FDA ì£¼ìš” ì •ì±… ë³€í™”:
            1. AI/ML ê¸°ë°˜ ì˜ë£Œê¸°ê¸° ê°€ì´ë“œë¼ì¸ ê°•í™”
            2. ì‚¬ì´ë²„ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ì˜ë¬´í™”
            3. Real-World Evidence í™œìš© í™•ëŒ€
            4. 510(k) ì‹¬ì‚¬ í˜„ëŒ€í™” í”„ë¡œê·¸ë¨

            êµ­ë‚´ ê¸°ì—… FDA ì¸í—ˆê°€ í˜„í™©:
            - 2024ë…„ FDA ìŠ¹ì¸ íšë“: 45ê±´
            - ì£¼ìš” ìŠ¹ì¸ ë¶„ì•¼: ì§„ë‹¨ê¸°ê¸°, AI ì˜ë£Œê¸°ê¸°, ìˆ˜ìˆ ë¡œë´‡

            ì§„ì¶œ ì „ëµ ì œì–¸:
            - ì´ˆê¸° ë‹¨ê³„ë¶€í„° FDA ê·œì œ ê³ ë ¤í•œ ê°œë°œ
            - Pre-submission ë¯¸íŒ… ì ê·¹ í™œìš©
            - í˜„ì§€ RA ì „ë¬¸ì¸ë ¥ í™•ë³´
            """,
            "ai_analysis": None,
            "crawled_at": datetime.now().isoformat()
        },
        {
            "id": 4,
            "title": "ì˜ë£Œê¸°ê¸° ê·œì œ ìƒŒë“œë°•ìŠ¤ ìš´ì˜ ì„±ê³¼ ë° ê°œì„  ë°©í–¥",
            "source": "ë³´ê±´ì‚°ì—…ë¸Œë¦¬í”„",
            "category": "ê·œì œ/ë²•ë ¹",
            "url": "https://www.khidi.or.kr/sample4",
            "content": """
            ì˜ë£Œê¸°ê¸° ê·œì œ ìƒŒë“œë°•ìŠ¤ëŠ” í˜ì‹  ì˜ë£Œê¸°ê¸°ì˜ ì‹ ì†í•œ ì‹œì¥ ì§„ì…ì„ ì§€ì›í•˜ëŠ” ì œë„ì´ë‹¤.

            ìš´ì˜ ì„±ê³¼ (2020-2024):
            - ì‹ ì²­ ê±´ìˆ˜: 320ê±´
            - ìŠ¹ì¸ ê±´ìˆ˜: 180ê±´ (ìŠ¹ì¸ë¥  56%)
            - ì‚¬ì—…í™” ì„±ê³µ: 45ê±´

            ì£¼ìš” ìŠ¹ì¸ ì‚¬ë¡€:
            1. AI ê¸°ë°˜ ì˜ë£Œì˜ìƒ ë¶„ì„ ì†Œí”„íŠ¸ì›¨ì–´
            2. ì›¨ì–´ëŸ¬ë¸” ê±´ê°• ëª¨ë‹ˆí„°ë§ ê¸°ê¸°
            3. ì›ê²©ì˜ë£Œ í”Œë«í¼

            ê°œì„  ê³¼ì œ:
            - ì‹¬ì‚¬ ê¸°ê°„ ë‹¨ì¶• (í˜„ì¬ í‰ê·  6ê°œì›” â†’ 3ê°œì›” ëª©í‘œ)
            - ì„ì‹œí—ˆê°€ í›„ ì •ì‹í—ˆê°€ ì „í™˜ìœ¨ ì œê³ 
            - ì‚¬í›„ê´€ë¦¬ ì²´ê³„ ê°•í™”
            """,
            "ai_analysis": None,
            "crawled_at": datetime.now().isoformat()
        },
        {
            "id": 5,
            "title": "ë³´ê±´ì‚°ì—… ì¸ë ¥ ìˆ˜ê¸‰ ì „ë§ ë° ì–‘ì„± ì „ëµ",
            "source": "ë‰´ìŠ¤ë ˆí„°",
            "category": "ì±„ìš© ë¶„ì„",
            "url": "https://www.khidi.or.kr/sample5",
            "content": """
            ë³´ê±´ì‚°ì—… ë¶„ì•¼ì˜ ì¸ë ¥ ìˆ˜ê¸‰ í˜„í™©ê³¼ ë¯¸ë˜ ì „ë§ì„ ë¶„ì„í•œë‹¤.

            í˜„ì¬ ì¸ë ¥ í˜„í™©:
            - ë³´ê±´ì‚°ì—… ì¢…ì‚¬ì: ì•½ 85ë§Œ ëª…
            - ì—°í‰ê·  ì¦ê°€ìœ¨: 4.2%
            - ì¸ë ¥ ë¶€ì¡± ë¶„ì•¼: AI í—¬ìŠ¤ì¼€ì–´, ë°”ì´ì˜¤ ë°ì´í„°, RA ì „ë¬¸ê°€

            2026ë…„ ìˆ˜ìš” ì „ë§:
            1. ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ì „ë¬¸ê°€: 5,000ëª… ì¶”ê°€ í•„ìš”
            2. ë°”ì´ì˜¤ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸: 2,000ëª… ì¶”ê°€ í•„ìš”
            3. ê¸€ë¡œë²Œ RA ì „ë¬¸ê°€: 1,500ëª… ì¶”ê°€ í•„ìš”

            ì¸ë ¥ ì–‘ì„± ì „ëµ:
            - ì‚°í•™í˜‘ë ¥ í”„ë¡œê·¸ë¨ í™•ëŒ€
            - ì¬ì§ì ì—­ëŸ‰ ê°•í™” êµìœ¡
            - í•´ì™¸ ìš°ìˆ˜ ì¸ë ¥ ìœ ì¹˜
            """,
            "ai_analysis": None,
            "crawled_at": datetime.now().isoformat()
        }
    ]

# ============================================================
# Streamlit UI
# ============================================================
def main():
    st.set_page_config(
        page_title="KHIDI AI ì±„ìš© ë¹„ì„œ",
        page_icon="ğŸ¥",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    # ì»¤ìŠ¤í…€ CSS
    st.markdown("""
    <style>
        .main-header {
            font-size: 1.8rem;
            font-weight: 700;
            color: #1a365d;
            margin-bottom: 0.5rem;
        }
        .sub-header {
            font-size: 1rem;
            color: #4a5568;
            margin-bottom: 2rem;
        }
        .briefing-card {
            background-color: #f8fafc;
            border-left: 4px solid #3182ce;
            padding: 1.5rem;
            margin-bottom: 1rem;
            border-radius: 0 8px 8px 0;
        }
        .category-tag {
            display: inline-block;
            padding: 0.25rem 0.75rem;
            background-color: #e2e8f0;
            border-radius: 9999px;
            font-size: 0.75rem;
            color: #4a5568;
            margin-right: 0.5rem;
        }
        .analysis-box {
            background-color: #fffbeb;
            border: 1px solid #f59e0b;
            border-radius: 8px;
            padding: 1.5rem;
            margin-top: 1rem;
        }
        .stTabs [data-baseweb="tab-list"] {
            gap: 8px;
        }
        .stTabs [data-baseweb="tab"] {
            padding: 10px 20px;
            background-color: #f1f5f9;
            border-radius: 8px 8px 0 0;
        }
        .stTabs [aria-selected="true"] {
            background-color: #3182ce;
            color: white;
        }
        .metric-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 12px;
            text-align: center;
        }
    </style>
    """, unsafe_allow_html=True)

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    init_database()

    # ========== ì‚¬ì´ë“œë°” ==========
    with st.sidebar:
        st.image("https://www.khidi.or.kr/resources/images/common/logo.png", width=180)
        st.markdown("---")

        st.markdown("### âš™ï¸ ì„¤ì •")

        # API í‚¤ ì…ë ¥
        api_key = st.text_input(
            "Gemini API í‚¤",
            type="password",
            help="Google AI Studioì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
        )

        if api_key:
            st.success("âœ… API í‚¤ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

        st.markdown("---")

        # ë°ì´í„° ìƒˆë¡œê³ ì¹¨
        st.markdown("### ğŸ”„ ë°ì´í„° ê´€ë¦¬")

        if st.button("ğŸ“¥ ìµœì‹  ë¸Œë¦¬í•‘ ìˆ˜ì§‘", use_container_width=True):
            with st.spinner("KHIDI ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘..."):
                collected = 0
                for board_name, board_url in KHIDI_URLS.items():
                    articles = crawl_khidi_board(board_name, board_url)
                    for article in articles:
                        content, pdf_url = get_article_detail(article['url'])

                        if pdf_url:
                            pdf_content = download_and_extract_pdf(pdf_url)
                            if pdf_content:
                                content = pdf_content

                        category = categorize_content(article['title'], content)

                        save_briefing(
                            title=article['title'],
                            source=board_name,
                            category=category,
                            url=article['url'],
                            pdf_url=pdf_url,
                            content=content[:5000] if content else None
                        )
                        collected += 1

                if collected > 0:
                    st.success(f"âœ… {collected}ê°œì˜ ë¸Œë¦¬í•‘ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.info("ìƒˆë¡œìš´ ë¸Œë¦¬í•‘ì´ ì—†ê±°ë‚˜ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        if st.button("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™”", use_container_width=True):
            if os.path.exists(DB_PATH):
                os.remove(DB_PATH)
            if os.path.exists(PDF_CACHE_DIR):
                import shutil
                shutil.rmtree(PDF_CACHE_DIR)
            init_database()
            st.success("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()

        st.markdown("---")
        st.markdown("### ğŸ“Œ ì•ˆë‚´")
        st.info("""
        **KHIDI AI ì±„ìš© ë¹„ì„œ**ëŠ” í•œêµ­ë³´ê±´ì‚°ì—…ì§„í¥ì›
        ì·¨ì—… ì¤€ë¹„ìƒì„ ìœ„í•œ ì¸ë°”ìŠ¤ì¼“ ë¶„ì„ ë„êµ¬ì…ë‹ˆë‹¤.

        - ìµœì‹  ë³´ê±´ì‚°ì—… ë™í–¥ì„ ìë™ ìˆ˜ì§‘
        - AIê°€ ì¸ë°”ìŠ¤ì¼“ ë‹µì•ˆ í˜•ì‹ìœ¼ë¡œ ë¶„ì„
        - ì±„ìš© íŠ¸ë Œë“œ ë° ìœ ë§ ì§ë¬´ ì˜ˆì¸¡
        """)

    # ========== ë©”ì¸ ì½˜í…ì¸  ==========
    st.markdown('<p class="main-header">ğŸ¥ KHIDI AI ì±„ìš© ë¹„ì„œ</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">í•œêµ­ë³´ê±´ì‚°ì—…ì§„í¥ì› ì¸ë°”ìŠ¤ì¼“ ë¶„ì„ ëŒ€ì‹œë³´ë“œ</p>', unsafe_allow_html=True)

    # ì˜¤ëŠ˜ ë‚ ì§œ
    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    st.markdown(f"**ğŸ“… {today}** ê¸°ì¤€ ë¸Œë¦¬í•‘")

    # ì¹´í…Œê³ ë¦¬ íƒ­
    tabs = st.tabs(CATEGORIES)

    for idx, tab in enumerate(tabs):
        category = CATEGORIES[idx]

        with tab:
            if category == "ì±„ìš© ë¶„ì„":
                render_recruitment_tab(api_key)
            else:
                render_briefing_tab(category, api_key)

def render_briefing_tab(category: str, api_key: str):
    """ë¸Œë¦¬í•‘ íƒ­ ë Œë”ë§"""

    # DBì—ì„œ ë°ì´í„° ì¡°íšŒ
    briefings = get_briefings(category)

    # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
    if not briefings:
        briefings = get_sample_briefings()
        if category != "ì „ì²´":
            briefings = [b for b in briefings if b['category'] == category]

    if not briefings:
        st.info(f"'{category}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë¸Œë¦¬í•‘ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for briefing in briefings:
        with st.container():
            st.markdown(f"""
            <div class="briefing-card">
                <span class="category-tag">{briefing.get('category', 'N/A')}</span>
                <span class="category-tag">{briefing.get('source', 'N/A')}</span>
                <h3 style="margin-top: 0.5rem; margin-bottom: 0.5rem;">{briefing['title']}</h3>
            </div>
            """, unsafe_allow_html=True)

            col1, col2 = st.columns([3, 1])

            with col1:
                # ì›ë¬¸ ë‚´ìš© í‘œì‹œ
                content = briefing.get('content', '')
                if content:
                    with st.expander("ğŸ“„ ì›ë¬¸ ë³´ê¸°", expanded=False):
                        st.markdown(content[:2000] + ("..." if len(content) > 2000 else ""))

            with col2:
                # AI ë¶„ì„ ë²„íŠ¼
                if st.button(f"ğŸ¤– AI ë¶„ì„", key=f"analyze_{briefing.get('id', briefing['title'][:10])}"):
                    if not api_key:
                        st.warning("ì‚¬ì´ë“œë°”ì—ì„œ Gemini API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        with st.spinner("AIê°€ ì¸ë°”ìŠ¤ì¼“ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ì¤‘..."):
                            analysis = generate_inbasket_analysis(
                                content=briefing.get('content', briefing['title']),
                                title=briefing['title'],
                                api_key=api_key
                            )
                            st.session_state[f"analysis_{briefing['title']}"] = analysis

            # ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            if f"analysis_{briefing['title']}" in st.session_state:
                st.markdown("---")
                st.markdown("### ğŸ¯ ì¸ë°”ìŠ¤ì¼“ ë¶„ì„ ê²°ê³¼")
                st.markdown(st.session_state[f"analysis_{briefing['title']}"])

            st.markdown("---")

def render_recruitment_tab(api_key: str):
    """ì±„ìš© ë¶„ì„ íƒ­ ë Œë”ë§"""

    st.markdown("## ğŸ“Š KHIDI ì±„ìš© ë¶„ì„ ì•„ì¹´ì´ë¸Œ")

    # ì±„ìš© ë°ì´í„° ì¡°íšŒ
    recruitment_data = get_recruitment_data()

    # ì—°ë„ë³„ í†µê³„
    col1, col2, col3, col4 = st.columns(4)

    years = [2022, 2023, 2024, 2025]
    for i, year in enumerate(years):
        year_data = [r for r in recruitment_data if r['year'] == year]
        total_hired = sum(r['hired_count'] for r in year_data)

        with [col1, col2, col3, col4][i]:
            st.metric(
                label=f"{year}ë…„",
                value=f"{total_hired}ëª…",
                delta=f"{len(year_data)}ê°œ ì§ë¬´"
            )

    st.markdown("---")

    # ì—°ë„ë³„ ì±„ìš© ìƒì„¸
    st.markdown("### ğŸ“‹ ì—°ë„ë³„ ì±„ìš© í˜„í™© (2021-2025)")

    year_filter = st.selectbox("ì—°ë„ ì„ íƒ", ["ì „ì²´"] + list(range(2025, 2020, -1)))

    if year_filter == "ì „ì²´":
        filtered_data = recruitment_data
    else:
        filtered_data = [r for r in recruitment_data if r['year'] == year_filter]

    if filtered_data:
        for data in filtered_data:
            with st.expander(f"**{data['year']}ë…„ | {data['position']}** ({data['department']})"):
                st.markdown(f"""
                - **ì±„ìš© ì¸ì›**: {data['hired_count']}ëª…
                - **ìê²© ìš”ê±´**: {data['requirements']}
                - **í•„ìš” ì—­ëŸ‰**: {data['skills']}
                """)

    st.markdown("---")

    # 2026ë…„ ìœ ë§ ì§ë¬´ ì˜ˆì¸¡
    st.markdown("### ğŸ”® 2026ë…„ ìœ ë§ ì±„ìš© ì§ë¬´ ì˜ˆì¸¡")

    if st.button("ğŸš€ AI ì˜ˆì¸¡ ìƒì„±", use_container_width=True):
        if not api_key:
            st.warning("ì‚¬ì´ë“œë°”ì—ì„œ Gemini API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            with st.spinner("AIê°€ 2026ë…„ ì±„ìš© íŠ¸ë Œë“œë¥¼ ë¶„ì„ ì¤‘..."):
                prediction = predict_future_jobs(api_key)
                st.session_state["job_prediction"] = prediction

    if "job_prediction" in st.session_state:
        st.markdown(st.session_state["job_prediction"])
    else:
        st.info("""
        **ì˜ˆì¸¡ ê¸°ë°˜ í‚¤ì›Œë“œ**: 2025 ë³´ê±´ì‚°ì—… ë°±ì„œ, ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´ ìœ¡ì„±ì „ëµ,
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… í˜ì‹ ì „ëµ, ì˜ë£Œê¸°ê¸° ì‚°ì—… ë°œì „ ë°©ì•ˆ

        'AI ì˜ˆì¸¡ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ 2026ë…„ ìœ ë§ ì§ë¬´ë¥¼ í™•ì¸í•˜ì„¸ìš”.
        """)

    st.markdown("---")

    # ì§ë¬´ë³„ íŠ¸ë Œë“œ ì‹œê°í™”
    st.markdown("### ğŸ“ˆ ì§ë¬´ë³„ ì±„ìš© íŠ¸ë Œë“œ")

    # ê°„ë‹¨í•œ ì°¨íŠ¸ ë°ì´í„°
    import pandas as pd

    trend_data = {
        "ì—°ë„": [2021, 2022, 2023, 2024, 2025],
        "R&D ì‚¬ì—…ê´€ë¦¬": [5, 4, 4, 5, 4],
        "ë””ì§€í„¸í—¬ìŠ¤": [0, 2, 6, 3, 3],
        "ê¸€ë¡œë²Œì§„ì¶œ": [0, 3, 0, 2, 0],
        "ì •ì±…ì—°êµ¬": [3, 2, 0, 0, 2],
    }

    df = pd.DataFrame(trend_data)
    df_melted = df.melt(id_vars=["ì—°ë„"], var_name="ì§ë¬´", value_name="ì±„ìš©ì¸ì›")

    st.bar_chart(df.set_index("ì—°ë„"))

if __name__ == "__main__":
    main()
